[{"authors":null,"categories":null,"content":"  1 Load Packages 2 Get the data 3 Wrangle and Estimate Percentage Changes 4 A bit of style 5 Plotting (aka the fun part ;) ) 5.1 Set the basic aesthetic mapping 5.2 Add the first geometric objects 5.3 Specify percent changes explicitly 5.4 Colors colors colors 5.5 Title axis and caption  6 Automate With a Function 7 Waterfall charts   Here I’ll show, you step by step, how to produce the plot in the tweet below. You can use it to show explicitly (and redundantly, but this is not always bad) absolute and relative changes among points in a time series.\nLate #TidyTuesday , #dataviz\nVisualization practice, I'm showing consumption of butter and its percent change compared to the previous year.\nI did this plot on “butter” just because its consumption changes.\nSource: https://t.co/yNp2qkOVO5\nCode: https://t.co/flNedKO1kh pic.twitter.com/yoTHspwBxZ — Otho (@othomn) February 1, 2019   To produce this plot, you just need to mix components from ggplot2, and the wonderful scico colour palette.\n1 Load Packages First you need to load the required R packages.\nlibrary(tidyverse) library(tibbletime) library(scico) The tidyverse is a collection of packages for data analysis, that contains the functions that we will use to load, manipulate and plot the data.\nThe package tibbletime stores the function rollify. This function allows you to apply another function, such as mean or sum over windows of time, or, as the name says, to “roll”\u0026quot; it.\nThe package scico stores wonderful color palettes, but more about that later.\n 2 Get the data I use this snippet of code to download data automatically and only once.\n# Get data Milk products --------------------------------------------------- dat_path \u0026lt;- \u0026quot;_data/2-05-milk-product-facts.Rdata\u0026quot; dat_url \u0026lt;- paste0(\u0026quot;https://raw.githubusercontent.com/\u0026quot;, \u0026quot;rfordatascience/tidytuesday/master/data/\u0026quot;, \u0026quot;2019/2019-01-29/milk_products_facts.csv\u0026quot;) if(!file.exists(dat_path)) { dat_milkprods \u0026lt;- read_csv(dat_url) save(dat_milkprods, file = dat_path) } else { load(dat_path) } The original data come from USDA, but this version has been already tidied by Thomas Mock as part of the of the TidyTuesday, a weekly social data project that he organizes. So we have little data manipulation left to do.\nThe milkprods dataset is already tidy, but it stores many columns that we don’t need.\ndat_milkprods %\u0026gt;% print() ## # A tibble: 43 x 18 ## year fluid_milk fluid_yogurt butter cheese_american cheese_other ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1975 247 1.97 4.73 8.15 6.13 ## 2 1976 247 2.13 4.31 8.88 6.63 ## 3 1977 244 2.34 4.29 9.21 6.78 ## 4 1978 241 2.45 4.35 9.53 7.31 ## 5 1979 238 2.44 4.49 9.60 7.57 ## 6 1980 234 2.50 4.47 9.62 7.90 ## 7 1981 230 2.44 4.24 10.2 8.03 ## 8 1982 224 2.58 4.35 11.3 8.60 ## 9 1983 223 3.16 4.91 11.6 8.96 ## 10 1984 224 3.55 4.98 11.9 9.62 ## # ... with 33 more rows, and 12 more variables: cheese_cottage \u0026lt;dbl\u0026gt;, ## # evap_cnd_canned_whole_milk \u0026lt;dbl\u0026gt;, evap_cnd_bulk_whole_milk \u0026lt;dbl\u0026gt;, ## # evap_cnd_bulk_and_can_skim_milk \u0026lt;dbl\u0026gt;, frozen_ice_cream_regular \u0026lt;dbl\u0026gt;, ## # frozen_ice_cream_reduced_fat \u0026lt;dbl\u0026gt;, frozen_sherbet \u0026lt;dbl\u0026gt;, ## # frozen_other \u0026lt;dbl\u0026gt;, dry_whole_milk \u0026lt;dbl\u0026gt;, dry_nonfat_milk \u0026lt;dbl\u0026gt;, ## # dry_buttermilk \u0026lt;dbl\u0026gt;, dry_whey \u0026lt;dbl\u0026gt;  3 Wrangle and Estimate Percentage Changes We need just 2 variables (year and butter) and we can easily extract them with dplyr::select.\nA bit more complex: we need to estimate percentage changes from the day before. This is swiftly done with rollify. We can use this function factory to build the function roll_percent(), which calculates percentage change over a column of a data frame.\n# Percent --------------------------------------------------------- # roll percent over a dataframe roll_percent \u0026lt;- rollify(.f = function(n) (n[2] - n[1])*100/n[1], 2) dat \u0026lt;- dat_milkprods %\u0026gt;% select(year, butter) %\u0026gt;% # apply on this dataframe, on the column butter mutate(percent = roll_percent(butter)) %\u0026gt;% filter(complete.cases(.)) So, this is the clean data frame that we use for plotting:\ndat %\u0026gt;% print() ## # A tibble: 42 x 3 ## year butter percent ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1976 4.31 -8.78 ## 2 1977 4.29 -0.441 ## 3 1978 4.35 1.41 ## 4 1979 4.49 3.14 ## 5 1980 4.47 -0.528 ## 6 1981 4.24 -5.13 ## 7 1982 4.35 2.67 ## 8 1983 4.91 12.8 ## 9 1984 4.98 1.42 ## 10 1985 4.87 -2.09 ## # ... with 32 more rows  4 A bit of style You can set the plot styles at any time, let’s do it now.\nBelow I modify the theme_minimal from ggplot2 with some fonts and colours that I like. I devised and and modified this from what Kieran Healy does.\ntheme_set( theme_minimal() + theme(text = element_text(family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 11), axis.title = element_text(size = 14), plot.title = element_text(colour = \u0026quot;grey20\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 18), plot.subtitle = element_text(face = \u0026quot;bold\u0026quot;, size = 12), aspect.ratio = .6, plot.margin = margin(t = 10, r = 15, b = 0, l = 10, unit = \u0026quot;mm\u0026quot;)) ) You can see setting a theme with theme_set() in ggplot2 as if you where applying a CSS file to your website. All plots below will be produced according to this theme.\n 5 Plotting (aka the fun part ;) ) Obviously, we will produce this plot with ggplot2.\n5.1 Set the basic aesthetic mapping First you can set the basic aesthetic mapping. All elements of the plot will have the variable year mapped to the x axis and butter mapped to the y axis.\nBelow, I also use a small dplyr trick to setting the yend variable, just before plotting. This variable doesn’t add anything to the dataset and I just need it when I plot the percentage changes, to make them look nice and clear.\nWith the tidyverse and ggplot2 you have at least two choices for setting variables on the fly:\n Right before plotting, in a pipe, as I’m doing here. Directly within ggplot2 when you define the aesthetic mapping, as I will do later.  p \u0026lt;- dat %\u0026gt;% mutate(yend = butter + (percent/10)) %\u0026gt;% ggplot(aes(x = year, y = butter)) At this point I’ve mapped basic aesthetic to the plot, but I did not specify any geometric object: this the plot is empty.\np %\u0026gt;% print()  5.2 Add the first geometric objects I’ll start with the grey annotation square, and the text, just because I want them to appear below any other object of the plot.\nThen I add the key geometric elements, the points, that are mapped to the absolute value of the butter sold in the US (lbs per person) and the arrows that are mapped to the percentage changes.\nNote: you can call aes() inside a call for a geometric object, as I do for geom_point(). In this way you can map a variable to an aesthetic parameter exclusively for that geometric object.\np \u0026lt;- p + # First the annotations annotate(geom = \u0026quot;rect\u0026quot;, xmin = 2008, xmax = 2010, ymin = -Inf, ymax = Inf, fill = \u0026quot;grey80\u0026quot;, alpha = .5) + annotate(geom = \u0026quot;text\u0026quot;, x = 2009, y = 4, label = \u0026quot;2008\\nEconomic Crisis?\u0026quot;, family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 3, fontface = \u0026quot;bold\u0026quot;) + # and then the basic geometric objects geom_segment(aes(yend = yend, xend = ..x.., colour = percent), size = 2, arrow = arrow(length = unit(1.2, \u0026quot;mm\u0026quot;), type = \u0026quot;closed\u0026quot;)) + geom_point(colour = \u0026quot;grey40\u0026quot;, size = 2) p %\u0026gt;% print()  5.3 Specify percent changes explicitly We can specify percent changes explicitly with numbers, in this way, readers can learn the specific size of the effect easily.\nI’ll do it with geom_text().\nThe numbers must appear above the arrow if the percentage change is positive, and, below, if it is negative. We achieve this using case_when(): a vectorized ifelse statement. I use case_when() directly in the call to the aesthetic mapping.\np \u0026lt;- p + geom_text(aes(y = case_when(percent \u0026gt; 0 ~ yend + .12, TRUE ~ yend - .12), label = percent %\u0026gt;% round() %\u0026gt;% paste0(\u0026quot;%\u0026quot;), colour = percent), size = 2.7) p %\u0026gt;% print()  5.4 Colors colors colors Colors: are they necessary? Are they to be avoided? The debate on what is the best way to use colors in a graph is wide and, well, colorful (forgive me ;) ). Well, colors, they make your plot look good, and for sure they can be helpful. How can we get the most out of them.\nBeside looking good, a colour palette today must have two properties:\n Be colour blind friendly (no need to explain this), Be perceptively uniform, or at least perceptively reasonable (i.e. it should not let you guess pattern that are not in the data).  The concept of perceptively uniform is explained clearly in the vignette of the viridis package.\nBeside the beautiful viridis palette, I also love those in the package scico. This package was developed by Thomas Lin Pedersen and it ports into R the color palettes developed by Fabio Crameri. I use the roma scale, which is divergent and colorful, and map percent change to it.\n# the roma palette scico::scico_palette_show(palettes = \u0026quot;roma\u0026quot;) I want the light and clear center of the divergent palette to be mapped to the 0% changes, so that the negative changes are red and the positive ones are blue. But percentage changes are not equally distributed around zero (since they are hopefully not random) so we can do it manually by setting artificial upper and lower limits to the colour mapping that are equally distant from zero.\nAfter doing that we can set the new color palette to the plot with the scale_colour_scico() function. Note that I have removed the lateral color guide with guide = FALSE, because it is not needed. In this way I free some space on the plot canvas.\n# a limit that centers the divergent palette lim \u0026lt;- dat$percent %\u0026gt;% range() %\u0026gt;% abs() %\u0026gt;% max() p \u0026lt;- p + scale_colour_scico(palette = \u0026quot;roma\u0026quot;, direction = 1, limits = c(-lim, lim), guide = FALSE) p %\u0026gt;% print()  5.5 Title axis and caption After that it’s just a matter of adding a title, a description, a caption and better labels for the axes.\nNote, in the original tweet I got the y axis label wrong. It should say “lbs per person”.\np \u0026lt;- p + labs(title = \u0026quot;Fluctuations in Butter Consumptions\u0026quot;, subtitle = str_wrap(\u0026quot;In the US between 1975 - 2017, with weight of sold butter in lbs and its percent change compared to the previous year.\u0026quot;), y = \u0026quot;Sold Butter in lbs per person\u0026quot;, x = \u0026quot;Year\u0026quot;, caption = \u0026quot;Data: USDA | Plot by @othomn\u0026quot;) p %\u0026gt;% print # Save ----------------------------------------------------------------- png(filename = \u0026quot;_plots/2-05-milk.png\u0026quot;, height = 1600, width = 2100, res = 300) p %\u0026gt;% print() dev.off()  ## png ## 2   6 Automate With a Function We can produce this plot from the original dataset with just one line of code if we place all steps in a function. I just leave out the title and subtitle that are different for each plot\nplot_arrows \u0026lt;- function(measure = butter) { # quote input measure \u0026lt;- enquo(measure) dat \u0026lt;- dat_milkprods %\u0026gt;% select(year, !!measure) %\u0026gt;% # apply on this dataframe, on the column butter mutate(percent = roll_percent(!!measure)) %\u0026gt;% filter(complete.cases(.)) # parameters to center the color palette lim \u0026lt;- dat$percent %\u0026gt;% range() %\u0026gt;% abs() %\u0026gt;% max() p \u0026lt;- dat %\u0026gt;% # Scale the arrow to the intensity of the measurement # In this way the arrow looks nice and clear in the plot # This step must be improved mutate(yend = !!measure + (percent)*(max(!!measure)/20)) %\u0026gt;% ggplot(aes(x = year, y = !!measure)) + # and then the basic geometric objects geom_segment(aes(yend = yend, xend = ..x.., colour = percent), size = 2, arrow = arrow(length = unit(1.2, \u0026quot;mm\u0026quot;), type = \u0026quot;closed\u0026quot;)) + geom_point(colour = \u0026quot;grey40\u0026quot;, size = 2) + geom_text(aes(y = case_when(percent \u0026gt; 0 ~ yend * 1.02, TRUE ~ yend * 0.97), label = percent %\u0026gt;% round() %\u0026gt;% paste0(\u0026quot;%\u0026quot;), colour = percent), size = 2.7) + scale_colour_scico(palette = \u0026quot;roma\u0026quot;, direction = 1, limits = c(-lim, lim), guide = FALSE) p } plot_arrows(fluid_milk) plot_arrows(cheese_other)  7 Waterfall charts Last but not least, Gina Reynolds suggested that one could misintepret the plots above, and understand that the mesurements points are at the top of the arrows instead than at the bottom.\nShe suggested that a waterfall chart could fix this issue and provide a more direct visualization, indeed a waterfall chart does its job.\nroll_prev \u0026lt;- rollify(.f = function(n) n[1], 2) dat \u0026lt;- dat %\u0026gt;% mutate(prev_year = roll_prev(butter)) %\u0026gt;% filter(complete.cases(.)) half_rect \u0026lt;- .3 p \u0026lt;- dat %\u0026gt;% mutate(yend = butter + (percent/10)) %\u0026gt;% ggplot(aes(x = year, y = butter)) + annotate(geom = \u0026quot;rect\u0026quot;, xmin = 2008, xmax = 2010, ymin = -Inf, ymax = Inf, fill = \u0026quot;grey80\u0026quot;, alpha = .5) + annotate(geom = \u0026quot;text\u0026quot;, x = 2009, y = 4.2, label = \u0026quot;2008\\nEconomic Crisis?\u0026quot;, family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 3, fontface = \u0026quot;bold\u0026quot;) + # geom_line(color = \u0026quot;grey80\u0026quot;) + geom_rect(aes(xmin = year - half_rect, xmax = year + half_rect, ymin = prev_year, ymax = butter, colour = percent, fill = percent), size = 0) + geom_segment(aes(x = year - half_rect, xend = ..x.. + 1 + half_rect*2, yend = ..y..), colour = \u0026quot;grey40\u0026quot;, size = 1) + geom_text(aes(y = case_when(percent \u0026gt; 0 ~ butter + .05, TRUE ~ butter - .05), label = percent %\u0026gt;% round() %\u0026gt;% paste0(\u0026quot;%\u0026quot;), colour = percent), size = 2.7) + scale_colour_scico(palette = \u0026quot;roma\u0026quot;, direction = 1, limits = c(-lim, lim), guide = FALSE) + scale_fill_scico(palette = \u0026quot;roma\u0026quot;, direction = 1, limits = c(-lim, lim), guide = FALSE) + guides(colour = element_blank()) + labs(title = \u0026quot;Fluctuations in Butter Consumptions\u0026quot;, subtitle = str_wrap(\u0026quot;In the US between 1975 - 2017, with weight of sold butter in lbs and its percent change compared to the previous year.\u0026quot;), y = \u0026quot;Sold Butter in lbs per person\u0026quot;, x = \u0026quot;Year\u0026quot;, caption = \u0026quot;Data: USDA | Plot by @othomn\u0026quot;) + theme_minimal() + theme(text = element_text(family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 11), axis.title = element_text(size = 14), plot.title = element_text(colour = \u0026quot;grey20\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 18), plot.subtitle = element_text(face = \u0026quot;bold\u0026quot;, size = 12), aspect.ratio = .6, plot.margin = margin(t = 10, r = 15, b = 0, l = 10, unit = \u0026quot;mm\u0026quot;)) p I think that the waterfall chart indeed solves this issue, but that the version with the uses the area in the canvas differently and stresses relative changes more.\nSo you can choose which chart to use depending on your needs. What do you think?\n ","date":1549065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549065600,"objectID":"35db3729fea9d3c77bd65d9914d9e3f3","permalink":"/post/2019-02-02-butter-consumption/","publishdate":"2019-02-02T00:00:00Z","relpermalink":"/post/2019-02-02-butter-consumption/","section":"post","summary":"Dataviz: some ggplot2 tricks and the wonderful scico color palette","tags":null,"title":"Show Absolute and Percent Change in a Time Series","type":"post"},{"authors":null,"categories":null,"content":"  1 Load Packages 2 Get the data 3 Wrangle and Estimate Percentage Changes 4 A bit of style 5 Plotting (aka the fun part ;) ) 5.1 Set the basic aesthetic mapping 5.2 Add the first geometric objects 5.3 Specify percent changes explicitly 5.4 Colors colors colors    Here I’ll show you step by step the plot in the tweet below. In this plot you can show explicitly (and redundantly, but this is not always bad) absolute and relative changes among points in a time series.\nLate #TidyTuesday , #dataviz\nVisualization practice, I'm showing consumption of butter and its percent change compared to the previous year.\nI did this plot on “butter” just because its consumption changes.\nSource: https://t.co/yNp2qkOVO5\nCode: https://t.co/flNedKO1kh pic.twitter.com/yoTHspwBxZ — Otho (@othomn) February 1, 2019   To produce this plot, you just need to mix components from ggplot2, and the wonderful scico colour palette.\n1 Load Packages First you need to load the required packages.\nlibrary(tidyverse) library(tibbletime) library(scico) The tidyverse is a collection of packages for data analysis, that contains the functions that we will use to load, manipulate and plot the data.\nThe package tibbletime stores the function rollify. This function allows you to apply another function, such as mean or sum over windows of time, or, as the name says, to “roll”\u0026quot; it.\nThe package scico stores wonderful color palettes, but more about that later.\n 2 Get the data I use this snippet of code to download data automatically and only once.\n# Get data Milk products --------------------------------------------------- dat_path \u0026lt;- \u0026quot;_data/2-05-milk-product-facts.Rdata\u0026quot; dat_url \u0026lt;- paste0(\u0026quot;https://raw.githubusercontent.com/\u0026quot;, \u0026quot;rfordatascience/tidytuesday/master/data/\u0026quot;, \u0026quot;2019/2019-01-29/milk_products_facts.csv\u0026quot;) if(!file.exists(dat_path)) { dat_milkprods \u0026lt;- read_csv(dat_url) save(dat_milkprods, file = dat_path) } else { load(dat_path) } The original data come from USDA, but this version has been already tidied by Thomas Mock as part of the of the TidyTuesday weekly social data project. So we have little data manipulation left to do.\nThe milkprods dataset is already tidy, but it stores many columns that we don’t need.\ndat_milkprods %\u0026gt;% print() ## # A tibble: 43 x 18 ## year fluid_milk fluid_yogurt butter cheese_american cheese_other ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1975 247 1.97 4.73 8.15 6.13 ## 2 1976 247 2.13 4.31 8.88 6.63 ## 3 1977 244 2.34 4.29 9.21 6.78 ## 4 1978 241 2.45 4.35 9.53 7.31 ## 5 1979 238 2.44 4.49 9.60 7.57 ## 6 1980 234 2.50 4.47 9.62 7.90 ## 7 1981 230 2.44 4.24 10.2 8.03 ## 8 1982 224 2.58 4.35 11.3 8.60 ## 9 1983 223 3.16 4.91 11.6 8.96 ## 10 1984 224 3.55 4.98 11.9 9.62 ## # ... with 33 more rows, and 12 more variables: cheese_cottage \u0026lt;dbl\u0026gt;, ## # evap_cnd_canned_whole_milk \u0026lt;dbl\u0026gt;, evap_cnd_bulk_whole_milk \u0026lt;dbl\u0026gt;, ## # evap_cnd_bulk_and_can_skim_milk \u0026lt;dbl\u0026gt;, frozen_ice_cream_regular \u0026lt;dbl\u0026gt;, ## # frozen_ice_cream_reduced_fat \u0026lt;dbl\u0026gt;, frozen_sherbet \u0026lt;dbl\u0026gt;, ## # frozen_other \u0026lt;dbl\u0026gt;, dry_whole_milk \u0026lt;dbl\u0026gt;, dry_nonfat_milk \u0026lt;dbl\u0026gt;, ## # dry_buttermilk \u0026lt;dbl\u0026gt;, dry_whey \u0026lt;dbl\u0026gt;  3 Wrangle and Estimate Percentage Changes We need just 2 variables and we can easily extract them with dplyr::select.\nA bit more complex: we need to estimate percentage changes from the day before. This is swiflty done with rollify. We can use this function factory to build the function roll_percent() calculates percentage change over a column of a dataframe.\n# Percent --------------------------------------------------------- # roll percent over a dataframe roll_percent \u0026lt;- rollify(.f = function(n) (n[2] - n[1])*100/n[1], 2) dat \u0026lt;- dat_milkprods %\u0026gt;% select(year, butter) %\u0026gt;% # apply on this dataframe, on the column butter mutate(percent = roll_percent(butter)) %\u0026gt;% filter(complete.cases(.)) So, this is the clean dataframe that we use for plotting:\ndat %\u0026gt;% print() ## # A tibble: 42 x 3 ## year butter percent ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1976 4.31 -8.78 ## 2 1977 4.29 -0.441 ## 3 1978 4.35 1.41 ## 4 1979 4.49 3.14 ## 5 1980 4.47 -0.528 ## 6 1981 4.24 -5.13 ## 7 1982 4.35 2.67 ## 8 1983 4.91 12.8 ## 9 1984 4.98 1.42 ## 10 1985 4.87 -2.09 ## # ... with 32 more rows  4 A bit of style You can set the plot styles at any time, let’s do it now.\nBelow I modify the theme_minimal from ggplot2 with some fonts and colours that I like. I devised this setting and modified them from what Kieran Healy does.\ntheme_set( theme_minimal() + theme(text = element_text(family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 11), axis.title = element_text(size = 14), plot.title = element_text(colour = \u0026quot;grey20\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 18), plot.subtitle = element_text(face = \u0026quot;bold\u0026quot;, size = 12), aspect.ratio = .6, plot.margin = margin(t = 10, r = 15, b = 0, l = 10, unit = \u0026quot;mm\u0026quot;)) ) See setting the theme with theme_set() in ggplot2 as if you where applying a CSS file to your website. All plots below will be produced according to this theme.\n 5 Plotting (aka the fun part ;) ) First you can set the basic aesthetic mapping. All elements of the plot will have the variable year mapped to the x axis and butter mapped to the y axis.\n5.1 Set the basic aesthetic mapping Below, I also use a small dplyr trick, setting the yend variable, just before plotting. This variable doesn’t add anything to the dataset, but I just need it when I plot the percentage changes, to make the look good.\nWith the tidyverse and ggplot2 you have at least two choices for setting variables on the fly:\n Right before plotting, in a pipe, as I’m doing here, Directly within ggplot2 when you define the aesthetic mapping, as I will do later.  p \u0026lt;- dat %\u0026gt;% mutate(yend = butter + (percent/10)) %\u0026gt;% ggplot(aes(x = year, y = butter)) At this point I’ve mapped basic aesthetic to the plot, but I did not specify any geometric object to appear, this the plot is empty.\np %\u0026gt;% print()  5.2 Add the first geometric objects I’ll start with the grey annotation square, and the text, just because I want them below any other object of the plot.\nThen I add the key geometric elements, the dots, that are mapped to the absolute value of the butter sold in the US (lbs per person) and the arrows that are mapped to the percentage changes.\nNote that you can call aes() inside a the call for a geometric object, such as geom_point(). In this way you can map a variable to an aesthetic exclusively in that geometric object.\np \u0026lt;- p + # First the annotations annotate(geom = \u0026quot;rect\u0026quot;, xmin = 2008, xmax = 2010, ymin = -Inf, ymax = Inf, fill = \u0026quot;grey80\u0026quot;, alpha = .5) + annotate(geom = \u0026quot;text\u0026quot;, x = 2009, y = 4, label = \u0026quot;2008\\nEconomic Crisis?\u0026quot;, family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 3, fontface = \u0026quot;bold\u0026quot;) + # and then the basic geometric objects geom_segment(aes(yend = yend, xend = ..x.., colour = percent), size = 2, arrow = arrow(length = unit(1.2, \u0026quot;mm\u0026quot;), type = \u0026quot;closed\u0026quot;)) + geom_point(colour = \u0026quot;grey40\u0026quot;, size = 2) p %\u0026gt;% print()  5.3 Specify percent changes explicitly We can specify percent changes explicitly with numbers, making it easier for readers to learn the specific size of the effect.\nI’ll do it with geom_text().\nThe numbers must appear above the arrow, if the percentage change is positive, and below if it is negaitve. We can specify this using case_when() a vectorized ifelse statement.I specify it directly in the call to the aesthetic mapping.\np \u0026lt;- p + geom_text(aes(y = case_when(percent \u0026gt; 0 ~ yend + .12, TRUE ~ yend - .12), label = percent %\u0026gt;% round() %\u0026gt;% paste0(\u0026quot;%\u0026quot;), colour = percent), size = 2.7) p %\u0026gt;% print()  5.4 Colors colors colors Colors: are they necessary? Are they to be avoided? Well, they make your plot look good, and for sure they are helpful. How can we get the most out of them.\nBeside looking good, a colour palette today must have two properties:\n Be colour blind friendly (no need to explain this), Be perceptively uniform, or at least perceptively reasonable (i.e. it should not let you guess pattern that are not there in the data).  The concept of perceptively uniform is explained clearly in the vignette of the viridis package.\nBeside the beautiful viridis palette, I also love those in the package scico. This package was developed by Thomas Lin Pedersen and it ports into R the color palettes developed by Fabio Crameri.\nI use the roma scale, which is divergent and colorful, and map percent change to it.\nI want the desaturated center of the divergent palette to be mapped to the 0% changes, but percentage data are not equally distributed around zero, so we can do it manually by setting artificial limits to the colot mapping.\n# needed to center divergent palette lim \u0026lt;- dat$percent %\u0026gt;% range() %\u0026gt;% abs() %\u0026gt;% max() p + scale_colour_scico(palette = \u0026quot;roma\u0026quot;, direction = 1, limits = c(-lim, lim), guide = FALSE) + guides(colour = element_blank()) + labs(title = \u0026quot;Fluctuations in Butter Consumptions\u0026quot;, subtitle = str_wrap(\u0026quot;In the US between 1975 - 2017, with weight of sold butter in lbs and its percent change compared to the previous year.\u0026quot;), y = \u0026quot;Sold Butter in lbs\u0026quot;, x = \u0026quot;Year\u0026quot;, caption = \u0026quot;Data: USDA | Plot by @othomn\u0026quot;) p # Save -------------------------------------------------------------------- png(filename = \u0026quot;plots/2-05-milk.png\u0026quot;, height = 1600, width = 2100, res = 300) p %\u0026gt;% print() dev.off()    ","date":1549065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549065600,"objectID":"a854ecb28be5e5a51260fa73b9564526","permalink":"/post/2019-02-02-butter.consumption/","publishdate":"2019-02-02T00:00:00Z","relpermalink":"/post/2019-02-02-butter.consumption/","section":"post","summary":"Dataviz: some ggplot2 triks and the wonderful scico color palette","tags":null,"title":"Show Absolute and Percent Change in a Time Series","type":"post"},{"authors":null,"categories":null,"content":"  1 Intro 2 A strategy for missing values [NA] 2.1 Ignore the NAs 2.2 A more solid strategy  3 Which Category is Overrepresented? 3.1 Exploratory plot 3.2 Hypergeometric test  4 Adjust plot for publication 5 Interpretation and closing remarks   1 Intro This is week 4 of the social data project TidyTuesday. In this week, we explore the incarceration trend dataset. This dataset stores US demographic and incarceration data by county and by gender / ethnic profile for the last 30 years.\nThe incarceration trend dataset is kindly provided by the Vera Institute on their Github page. Analyzing this dataset aims at remembering of the social injustices still present in our world on Martin Luther King Jr. Day.\nFor this analysis, I want to practice modelling rather than data wrangling and tidying, so I decided to start from the file prison_population.csv, from Tidytuesday’s github page which has already been tidied by Thomas Mock.\n# Setup ------------------------------------------------------------------- library(tidyverse) theme_set(theme_bw()) # Get Data ---------------------------------------------------------------- # download data directly from github and store them as Rdata locally. dat_path \u0026lt;- \u0026quot;_data/2-04-prison.Rdata\u0026quot; dat_url \u0026lt;- paste0(\u0026quot;https://raw.githubusercontent.com/\u0026quot;, \u0026quot;rfordatascience/tidytuesday/master/data/\u0026quot;, \u0026quot;/2019/2019-01-22/prison_population.csv\u0026quot;) if(!file.exists(dat_path)) { dat \u0026lt;- read_csv(dat_url) save(dat, file = dat_path) } else { load(dat_path) }  2 A strategy for missing values [NA] Collecting such detailed data is a massive effort, and some missing values are inevitable. For statistical modeling we must select a clear and explicit strategy to deal with measurements that are mixed with missing values.\nThe dataset prison_population.csv has many missing values stored as NA in the column prison_population. That column counts incarcerated individuals, and it is our main interest together with the population columns that stores a full population census.\n# how many NAs in the variable prison_population? dat$prison_population %\u0026gt;% is.na() %\u0026gt;% sum() ## [1] 751787 2.1 Ignore the NAs A very basic strategy, could be just to ignore the NAs. We can try to plot a quick summary of prison population by state, using sum(na.rm = TRUE) to ignore NAs. But this strategy, introduce strange fluctuations in the measurements, because randomly occurring NAs increase or decrease the prison population counts with patterns that are not reflected in reality.\n# Try sum(na.rm = TRUE) -------------------------------------------------------- # summarize the data by state and year dat_state \u0026lt;- dat %\u0026gt;% filter(pop_category == \u0026quot;Total\u0026quot;, year \u0026gt; 1982, year != 2016) %\u0026gt;% group_by(year, state) %\u0026gt;% summarise(prison_population = prison_population %\u0026gt;% sum(na.rm = TRUE), population = population %\u0026gt;% sum(na.rm = TRUE)) # plot them as an heatmap dat_state %\u0026gt;% ggplot(aes(x = year, y = state, fill = prison_population)) + geom_raster() + scale_fill_viridis_c(trans = \u0026quot;log10\u0026quot;, breaks = 10^(1:5)) + # Do not add padding around x limits scale_x_continuous(expand = expand_scale(0)) ## Warning: Transformation introduced infinite values in discrete y-axis We can hypothesize that the sharp changes in the prison_population variable are caused by missing data, rather than by real changes in the population of prisons.\n 2.2 A more solid strategy As a more solid strategy to deal with missing values, we can keep only measurements from counties in which the variable prison_population never has missing values.\nFor example, if in a county we did not count the prison population for two years, and we thus have missing values, when we sum the data for those county to the others ignoring NAs, we would notice a sharp increase and decrease in prison population, that is not reflected in reality.\nIt is better to remove measurements from that county altogether. In this way we may lose some measurements. But the time series that we retain reflects better the real trends in prison population.\n(If we needed to retain more measurements, we could have tried to impute missing values, but in this case we don’t need to. Because we should have already enough measurements to make insightful observations).\nWe can use dplyr to create the new variable has_na that is TRUE if any measurement from that county contains missing values. And then we can use to filter out those observations.\ndat_clean \u0026lt;- dat %\u0026gt;% filter(pop_category == \u0026quot;Total\u0026quot;, # to include more counties, I have reduced the time span year \u0026gt;= 1990, year != 2016) %\u0026gt;% group_by(state, county_name) %\u0026gt;% mutate(has_na = anyNA(prison_population)) %\u0026gt;% filter(!has_na) %\u0026gt;% ungroup() Let’s do again the heatmap, but after we have removed counties with missing values.\ndat_clean %\u0026gt;% # first summarize data by state and year group_by(year, state) %\u0026gt;% summarise(prison_population = sum(prison_population), population = sum(population)) %\u0026gt;% # then plot the heatmap ggplot(aes(x = year, y = state, fill = prison_population)) + geom_raster() + scale_fill_viridis_c( # trans = \u0026quot;log10\u0026quot;, breaks = 10^(1:5) ) + # Do not add padding around x limits scale_x_continuous(expand = expand_scale(0)) (If you compare this heatmap with the one before, you’ll notice that here the colour scale is mapped to a linear scale, rather then a log scale, because in this case the differences are so smooth that they get imperceptible in log scale.)\nNow the progression through time is much smoother.\nAs you notice, I have restricted the time span from 1990 to 2015 to retain more counties. Nevertheless, we have lost information about many states.\n# Which State is missing in the clean dataset? setdiff(dat$state, dat_clean$state) ## [1] \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;MT\u0026quot; ## [15] \u0026quot;NV\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;VT\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;WY\u0026quot; We can anyway go on, apply this system to remove NA to the observations split by gender and ethnic categories, and test on that dataset which category is overrepresented in prison population.\n  3 Which Category is Overrepresented? First, we can clean from missing values the observations that are split by categories.\n# Try with more details --------------------------------------------------- by_cat \u0026lt;- dat %\u0026gt;% filter( pop_category != \u0026quot;Other\u0026quot;, year \u0026gt;= 1990, year != 2016) %\u0026gt;% group_by(state, county_name, pop_category) %\u0026gt;% mutate(has_na = anyNA(prison_population)) %\u0026gt;% filter(!has_na) %\u0026gt;% ungroup() 3.1 Exploratory plot Then we can produce some plots to explore how the various categories behave. In this case I’ve found this boxplot most helpful.\nby_cat %\u0026gt;% mutate(ratio = prison_population/population) %\u0026gt;% ggplot(aes(x = year, y = ratio, group = year)) + geom_boxplot() + scale_y_log10() + facet_grid(pop_category ~ ., scales = \u0026quot;free\u0026quot;, # Wrap the text in the strip labels labeller = label_wrap_gen(width = 10)) ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 47965 rows containing non-finite values (stat_boxplot). Above, we can observe that:\n The ratio of African american [Black] incarcerated is higher than any other category, with median around 0.02. The ratio of males incarcerated is very high, with median going from 0.01 to 0.02. The ratio of Latino and Native American incarcerated are also high.  Just to be sure, we can also reproduce the same heatmap as above on all categories.\nby_cat %\u0026gt;% mutate(ratio = prison_population/population) %\u0026gt;% ggplot(aes(x = year, y = state, fill = prison_population)) + geom_raster() + facet_grid(. ~ pop_category) + scale_fill_viridis_c(trans = \u0026quot;log10\u0026quot;, breaks = 10^c(1:5)) + # because of facetting, the x axis is very tight theme(axis.text.x = element_text(angle = 90, vjust = .5)) ## Warning: Transformation introduced infinite values in discrete y-axis We can see that data are still sparse, but importantly, for each year/category we have sets of paired observation of prison_population and population without missing values.\n 3.2 Hypergeometric test We can model this data with an hypergeometric distribution, and use it to test if a category of gender or ethnicity is overrepresented.\nFirst, we can (again) summarize observation by state and year (we don’t want to test over-representation for every county).\nby_cat_sum \u0026lt;- by_cat %\u0026gt;% filter(pop_category != \u0026quot;Other\u0026quot;) %\u0026gt;% group_by(pop_category, year) %\u0026gt;% summarise(population = sum(population), prison_population = sum(prison_population)) %\u0026gt;% ungroup() Then we have to prepare the data for the function phyper() that will estimate the p-value of each observation under the hypergeometric distribution.\nAs state by its help page, the phyper() function requires these parameters:\n  q: vector of quantiles representing the number of white balls drawn without replacement from an urn which contains both black and white balls. m: the number of white balls in the urn. n: the number of black balls in the urn. k: the number of balls drawn from the urn.     From the help pages of the stats package\n  We can reshape the dataset and place each of those values in a separate column. If we call each column as the appropriate parameter of the function phyper(), then we loop this function on each row of the dataset with pmap() and match all arguments automatically by name.\n# prepare a table for hyopergeometric test: # get category total next to each other category by_cat_tot \u0026lt;- by_cat_sum %\u0026gt;% filter(pop_category == \u0026quot;Total\u0026quot;) %\u0026gt;% rename_all(funs(paste0(., \u0026quot;_total\u0026quot;))) by_cat_hyp \u0026lt;- by_cat_sum %\u0026gt;% left_join(by_cat_tot, by = c(\u0026quot;year\u0026quot; = \u0026quot;year_total\u0026quot;)) # apply phyper() using pmap # Define phyper() wrapper that contains \u0026quot;...\u0026quot; # So that it can be used in pmap with extra variables # Test enrichment # inspired from # https://github.com/GuangchuangYu/DOSE/blob/master/R/enricher_internal.R phyper2 \u0026lt;- function(q, m, n, k, ...) { phyper(q, m, n, k, log.p = TRUE, lower.tail = FALSE) } by_cat_hyp \u0026lt;- by_cat_hyp %\u0026gt;% # rename arguments for dhyper transmute(year = year, pop_category = pop_category, q = prison_population, # white balls drawn # x = prison_population, # white balls drawn m = population, # white balls in the urn n = population_total - population, # black balls in the urn k = prison_population_total) # balls drawn from the urn This approach was inspired by the field of genomics and transcriptomics, in which the hypergeometric test is often used to test the if structural or functional categories of genes are enriched in a given set. Some code here is inspired by this bioconductor package\nThen we can use pmap() to run an hypergeometric test on each row.\n# apply dhyper() to every row by_cat_hyp \u0026lt;- by_cat_hyp %\u0026gt;% mutate(log_p = pmap(., .f = phyper2) %\u0026gt;% purrr::flatten_dbl()) And eventually we can plot the log p-values with inverse sign, to make the plot more intuitive.\np \u0026lt;- by_cat_hyp %\u0026gt;% # I could have filtered out this earlier, # but it served as practical control filter(pop_category != \u0026quot;Total\u0026quot;) %\u0026gt;% # filter categories not overepresented filter(log_p \u0026lt; -100) %\u0026gt;% ggplot(aes(x = year, y = -log_p)) + geom_bar(stat = \u0026quot;identity\u0026quot;, fill = \u0026quot;orange\u0026quot;, colour = \u0026quot;black\u0026quot;) + facet_grid(pop_category ~ .) p %\u0026gt;% print() We can see that the categories “Black” and “Male” are far from what we would expect by chance, and, thus, members of that categories are overrepresented.\n  4 Adjust plot for publication We can adjust the plot labels for publication. Adding clearer labels, a title, and making small adjustments to the layout.\np2 \u0026lt;- p + labs(title = \u0026quot;Categories that are Overrepresented in US Prisons\u0026quot;, subtitle = str_wrap(\u0026quot;A quick exploratory analysis of the VERA dataset, using a hypergeometric test to estimate which category is more represented than expected\u0026quot;), width = 27, y = \u0026quot;-log(p-value)\u0026quot;, x = \u0026quot;Year\u0026quot;, caption = \u0026quot;Source: www.vera.org | Plot by @othomn\u0026quot;) + theme(text = element_text(family = \u0026quot;Arial Narrow\u0026quot;, colour = \u0026quot;grey40\u0026quot;, size = 11), axis.title = element_text(size = 14), strip.text = element_text(colour = \u0026quot;grey20\u0026quot;, size = 14), plot.title = element_text(colour = \u0026quot;grey20\u0026quot;, face = \u0026quot;bold\u0026quot;, size = 18), plot.subtitle = element_text(face = \u0026quot;bold\u0026quot;, size = 12), aspect.ratio = .2, plot.margin = margin(t = 10, r = 10, b = 0, l = 3, unit = \u0026quot;mm\u0026quot;)) p2 %\u0026gt;% print()  5 Interpretation and closing remarks This is a quick analysis and my take on Tidytuesday week 4 dataset.\nMy analysis shows an issue that is widely known, that African American are overrepresented in US prisons. But this just a statistical analysis, which could be helpful or misleading if not contextualized. If you have any suggestion on how to improve my analysis, please contact me.\nUnfortunately, in here I don’t contextualize and I don’t discuss this results. If you are interested in this topic, if you feel engaged by these results, and you want to know more. If you want to interpret this data, you’ll have to contextualize this results. To do so, you’ll have to read about history and social issues! And if you are interested and you want to form an opinion, please, please, please, do read, explore and contextualize.\nMany thanks to Thomas Mock for bringing the work of the Vera institute to our attention on Martin Luther King Jr. Day.\n ","date":1548374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548374400,"objectID":"cfb6b81a4c721054682838070c51368a","permalink":"/post/2-04-prison/","publishdate":"2019-01-25T00:00:00Z","relpermalink":"/post/2-04-prison/","section":"post","summary":"How to deal with missing values, exploratory plots and heatmaps, and the hypergeometric test in R.","tags":null,"title":"An analysis of Vera institute's prison dataset","type":"post"},{"authors":null,"categories":null,"content":"An R package to analyze Rice panicle architecture.\n","date":1544396400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544396400,"objectID":"d4c5c1e1e966aab055a6a29b393eff39","permalink":"/project/ptrapr/","publishdate":"2018-12-10T00:00:00+01:00","relpermalink":"/project/ptrapr/","section":"project","summary":"An R package to analyze Rice panicle architecture.","tags":null,"title":"PTRAPR","type":"project"},{"authors":["Florian Hahn","Marion Eisenhut","Otho Mantegazza","Andreas PM Weber"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"3e4e11f932c467e94cf6a6d0b01ecf88","permalink":"/publication/hahn-2018-homology/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/hahn-2018-homology/","section":"publication","summary":"","tags":null,"title":"Homology-directed repair of a defective glabrous gene in Arabidopsis with Cas9-based gene targeting","type":"publication"},{"authors":null,"categories":null,"content":" During the year I turned some plot that I like into postcards. If you like them, feel free to print them and give them as a gift.\nThe source code is here.\nTartan AKA: R packages downloaded from CRAN.\nIn green:\nAnd in red:\nGalaxy AKA: Gene Expression Correlation.\nIn white:\nAnd in blue:\nChristmas Tree AKA: A rice panicle.\nIn blue:\nAnd in red:\n","date":1501538400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501538400,"objectID":"b911c8cec06fdb528722641a77dc4837","permalink":"/project/postcards/","publishdate":"2017-08-01T00:00:00+02:00","relpermalink":"/project/postcards/","section":"project","summary":"Some graph in R turned postcards. If you like them, feel free to print them and give them as a gift. :)","tags":null,"title":"Postcards","type":"project"},{"authors":null,"categories":null,"content":"","date":1501538400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501538400,"objectID":"58aa2469c46466abcb8f6557982ab30c","permalink":"/project/ps-booklet/","publishdate":"2017-08-01T00:00:00+02:00","relpermalink":"/project/ps-booklet/","section":"project","summary":"A booklet about photosynthesis and how we study it.","tags":null,"title":"What does the plant do?","type":"project"},{"authors":null,"categories":null,"content":"  1 Back To The Books 2 Tools For Writing Open Books on Github 2.1 Markdown 2.2 Git and Github 2.3 Putting it together: Books, authorship and contributions 2.4 Bonus: Bookdown  3 Publishing 3.1 Own website 3.2 Rstudio connect, through Bookdown 3.3 Publishers  4 Impact 4.1 Paywall 4.2 Updates and crowdsourcing  5 Wrapping it up and what’s next   1 Back To The Books Toward the end of my PhD I was struggling to learn new skills such as data analysis and programming. It was at that time that I rediscovered books (duh!) and discovered a very special kind of them, known as open books or open source books. This kind of books helped me in my professional development, so I feel particularly attached to them.\nBy now I have found three types of open source books:\n Books that are simply distributed by the author on their own website. Books that are written through wikis, known as wikibooks. Books that are written through Github.  Here I will discuss the last type, because I have the impression that Github is a powerful tool for writing books that also fits perfectly to academics.\nA disclaimer first: I come here as a beginner. My area of expertise is biology and I have never written a book about it, but I would really like to do so. Meanwhile I collected the necessary tools and some idea on why we should do it. And I would like to share them with you.\n 2 Tools For Writing Open Books on Github It all revolves around two tools: Markdown and, you have guessed it, Github.\n2.1 Markdown Markdown is a simple markup language; we can use it for annotating plain text, so that we know which part is the title, which part should be bold or italic, which part makes up paragraph, etc.\nThe most widely used markup languages are HTML and LATEX. Compared to them markdown is less versatile, but incredibly simple and easy to use.\nFor example, in markdown:\n# This is a title ## This is a subtitle This is a paragraph. This text is **bold**, and this text is *italic* This is a bullet list: - item1 - item2 [This is a link](to/your/favourite/URL) etc.\nA big chunk of the web, for example this blog and many others, is written in Markdown.\nOnce you have written markdown text what can you do with it?\nMarkdown is a universal source that can be easily converted into nicely formatted docx and pdf, but also into HTML and epub/mobi. You can convert any markdown file into any of those formats with pandoc , no edits required.\nSince a markdown file is plain text, you can open it on any computer and any platform, you just need a text editor (I generally use atom). It is not even worth mentioning that pandoc and atom are open source.\nAlso, since markdown is plain text, it can be tracked with Git and hosted on Github. (if you already know what they are, skip the next section)\n 2.2 Git and Github Git is a version control software. It was designed for software developers that need to keep track of what they write and delete while they develop their software; and also to collaborate while doing this. Git is a tool that is somehow complementary to a backup, it keeps track of changes to specific files on a project-wise schedule.\nBy tracking the modifications to a file, Git assigns exact authorship to each line of such file. So Git, plus its online hub Github allows you to publish, update and track any modification (and authorship of those modifications) of files, online and in an open way. This features make Git a great tool for open but controlled collaboration.\nSince the source code of programs is indeed text, Git works perfectly also for writing books (and blog, websites, manual etc.), as long as they are written in plain text formats, such as markdown.\n 2.3 Putting it together: Books, authorship and contributions A combination of Markdown (universal source) and Github (publish, author, track and collaborate) is great for publishing open source books. Many academics such as Hadley Wickham are already using it.\n2.3.1 Authorship and contributions We academics care a great deal that our work gets accredited to us, and only because we are proud of it: this is very important for us in order to keep our job or to the next one. So, I would like to take one of Hadley Wickham’s books as example to discuss authorship and contributions.\nAs you can see, this book is tracked on Github. Github does not only tracks and keeps records of any file that constitute the book, it also keeps records of the authorship of every line of those files. As you can see, Hadley Wickham authored most of the book himself, but many other people provided small contribution. For every contributor, you can tell exactly what they wrote or deleted in the various files that constitute the source for that book. What you can’t see is that each of these contribution had to be approved from the main author.\nSo, a combination of markdown and Github allows you to self publish your own books. Moreover, this system provides strong authoring tools: anybody can propose changes / contributions to your book, and every contribution must be approved by you (or by any of the authors that have such privilege).\nBe aware, since your book now is an open source project, it can be forked\n  2.4 Bonus: Bookdown An experienced user can rely on its own skills to convert all the markdown files that constitute a book into a nicely formatted web page, the good news is that we don’t have to do this by ourself: instead we can use Bookdown\nBookdown is an R package that renders markdown books into HTML pages and many other formats. Bookdown’s manual is written in Bookdown itself, so you can peek how the rendering looks like. Since Bookdown is written in R, it is most widely known to the R community. Indeed most of the book written in Bookdown are about R, but they don’t need to, using Bookdown requires very little knowledge of R.\nAnother powerful tool is Gitbook, it was used to author this collection of technical books\n  3 Publishing Open source books can be published in many ways, often one way does not exclude the others.\n3.1 Own website Markdown and Pandoc provide an HTML output, so your book can be easily integrated into any website, manually or automatically. For example, travis-ci can build your book automatically from Github and push it to your website; but I have not checked the details of how to do this, and setting this up might not be easy.\nIn this way, you fully control the copyright and the license to your book (although it is reasonable to use Github together with open source or share-alike licenses).\n 3.2 Rstudio connect, through Bookdown Bookdown provides an integrated system to publish your book through Rstudio connect to its website. This is detailed on the “Publishing” chapter of bookdown’s manual together with other extensive information on publishing.\n 3.3 Publishers It is also possible to sell open source books through publishers. Among those Leanpub does a good job, I personally purchased books there. Leanpub accepts to sell open source books written in markdown directly from Github. Their publishing process is fast; indeed they accept to distribute your book in real time while you write it, providing updates at anytime to the customers.\nMost of the books on Leanpub are about programming, but it does not have to be like this, we could publish there books on biology, or about any of your fields of interest.\nIf your open source book is incredibly good, you can also sell printed versions through traditional publishers. Indeed Hadley Wickham sells his books through Chapman \u0026amp; Hall.\n  4 Impact A small note: I haven’t written or published an open source book yet; I wrote this section starting from my personal observations and experience while studying and working in academia in the last years.\n4.1 Paywall We scientists often complain about how people don’t rely on the knowledge that we produce during years. But how much of this knowledge is locked behind a paywall that has questionable reasons to exist? Writing open source books, at least while we are funded with public money, could have an ethical side.\nWhat about the practical side? We academics do not earn huge amounts of money for writing books that are often sold at high prices. If we would distribute open source books instead, we would not lose much. And we could gain on impact: an open source book can be accessed worldwide, and it could be also easily translated.\nBeside, remember that open source books can always be sold through Leanpub.\n 4.2 Updates and crowdsourcing Imagine the impact of a book that can be freely:\n accessed online downloaded on a e-reader printed  And that can also be updated in real time. When we specialize we sometimes feel that books gets useless because they quickly become obsolete. Well, open source books do not.\nMoreover they are less of a burden on the author, because readers can contribute to the book and improve it, from fixing typos to writing whole sections. For example, the author of Bookdown, (Yihui Xie), is also writing an R package to make website (that, by the way, I am using to make this blog). The manual for this new package is being written with Bookdown and many people are already contributing to it.\nWhat better peer review than the readers themselves?\n  5 Wrapping it up and what’s next Markdown and Github are powerful tools to publish open source books. Markdown books can be easily compiled to very nicely formatted HTML books using packages such as Bookdown. Github enables open but controlled collaboration and provides strong authorship tools.\nWith this system we could publish beginner-level books or also short high level reference books on the our topics of interest. Our readers could be University students or any person that want to learn such topics.\nIf academics would engage in this activity they could have a great impact on society.\nI don’t know if I am expert enough to write an open source book on any of my area of interest. But it sounds like a good long term project and I look forward to do it. The tools are all there.\n ","date":1500163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500163200,"objectID":"3c2b1045eb394f9c12d63e1810b3539b","permalink":"/post/2017-07-16-back-to-the-books/","publishdate":"2017-07-16T00:00:00Z","relpermalink":"/post/2017-07-16-back-to-the-books/","section":"post","summary":"During my PhD I was struggling to learn new skills, then I discovered open books, available in HTML and on Github","tags":null,"title":"Back To The Books","type":"post"},{"authors":null,"categories":null,"content":" Can Open Source Hardware help fill the gap and bring technology within reach for everybody?\nI am a biologist in academia. I am curious about how Open Source Hardware can improve the way we do research and the way in which we distribute our results and the technology that we produce.\nDefinitions first The Open Source Hardware Association defines Open Source Hardware as:\n “hardware whose design is made publicly available so that anyone can study, modify, distribute, make, and sell the design or hardware based on that design”\n Starting from the late 90s many engineers, passionate users, companies and organization have tried to define standards for Open Source Hardware and to advertise for it, they were probably inspired by the success of Open Source Software.\n Lessons from Software A couple of decades ago, the Open Source movement made a strong statement on how the source code of commonly used software should be available to everybody.\nThis was a matter of fairness. We deal with software constantly in our life and it sounds reasonable that we know how that software works.\nBut this proved also practical. It fueled software development in the last decades and the broad start-up environment that thrived around it.\nIt is worth mentioning that sharing the source code is good enough, but many open source projects went further than that. They provided extensive and sometimes redundant documentation on how to use their product, on how to take control of it and on how to modify any aspect of it. Then, the community could redistribute the modified and improved products thanks to permissive and copyleft licenses.\nThe success of this model highlights the benefits that arise from a widespread and distributed access to knowledge for everybody.\n In Universities and in biology Through synthetic biology, open source hardware is already making its way to biology and biotechnology.\nI am trying to collect a list of projects that are related to Open Source Hardware and Biology and looking at homemade centrifuges, open PCR machines, microfluidics devices, I have the impression that this way of doing things is rapidly growing.\nAs stated above, I am interested in its role and impact on academic research, specifically in the fields of biology and plant science. (and eventually I would like to use those tools myself for my research :) )\nRedistributing knowledge By definition Open Source software and hardware are about how we distribute knowledge.\nA model that stresses the importance of openly redistributing knowledge as widely as possible and of facilitating re-use of that knowledge / technology seems only reasonable for Universities, which indeed have the role in society of generating knowledge and making it available.\nWhat I am also really curious about is to figure out if Open Source Hardware could also improve the way we do research and not exclusively the way we distribute it.\n Making research easier for academics In my (small) experience as academic researcher, one of the main issues that we, young biologists, face is that we are mostly end users of very expensive and sophisticated machines and technologies.\nIn this way, we can perform very complex tasks and experiments with relatively little effort, but we also partially lose control on what we are doing.\nIn details, this might cause that:\n Laboratories depend more and more on high level funding; low funded labs get left behind, which could be an issue on the global scale. Since the end user model is productive in the short term, laboratories that try different approaches, again, might get left behind, This relegates researchers to a performer / technical position, in which they are required to perform a high number of repetitive tasks with little chances to modify the workflows of their experiments. If researchers would gain more control on the technology that they use, they might have bigger chances of optimizing their experimental workflows. For example, making them simpler, less polluting and more accessible. This might have an effect also on reproducibility and distributed collaborative development, simply because the chances of making reproducible something on which you have no control are small.  Open Source Hardware might have a role solving those issues but since only few laboratories implement open hardware methodically I can’t be sure about it.\nBut open source software already helped biologist a lot, especially in bioinformatics and in data analysis through impressive frameworks such as, for example, BioConductor. This raises hopes that Open Source Hardware could have a similar impact for wet lab scientists.\nOf course the issue of a funding system that might encourage bad scientific behavior remains, but Open Hardware could be a move in the right direction and improve the life of young biologists.\n  An Impact on farming? As academic researcher I often wonder what will be the impact of my research.\nI am specialized in genetics and plant biology, so my research might have impact on agriculture and farming, especially in breeding new crop varieties.\nIf we, plant biologists, would apply Open Source Hardware systematically, we might, for example, improve farming by lowering the costs necessary to access technologies such as plant breeding.\nSo, can Academic Research improve its impact, down the line, on farming by adopting an open knowledge / open source software and hardware model? And also, could it help shift the role of a farmer from end user to user/developer?\nSome organizations are already investing on this model. I am moving a little bit outside of my field of expertise right here, so I might not be the best person to answer those questions; but as always, the success of Open Source Software raises hope that yes, this can happen.\nThanks to Silvia, Leone and Alessandro for the help and the ideas.\n ","date":1495756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495756800,"objectID":"bfd733ee0badc032c8b3f05195a41475","permalink":"/post/2017-05-26-fill-the-gap/","publishdate":"2017-05-26T00:00:00Z","relpermalink":"/post/2017-05-26-fill-the-gap/","section":"post","summary":"Can Open Source Hardware help fill the gap and bring technology within reach for everybody?","tags":null,"title":"Fill the Gap","type":"post"},{"authors":null,"categories":null,"content":"As a young scientist, I find myself facing these challenges (or, at least, let\u0026rsquo;s say that I am trying to improve in these areas):\n make research in an interdisciplinary and distributed context, maximize the chances that anybody can access and reproduce my results, engage with the wide public.  Learning these enabling tools are helping me dealing with this:\n A programming language, easier if it is a high level one such as R or Python. Knowing a programming language not only helps you get a computer or a robot do what you want. It also allow you to write and share with everybody a rigorous set of instruction (scripts) for what you do, A markup language, such as Latex, or a combination of HTML and markdown, this enables you to easily publish what you want on the web. It can be a blog, a text book, or an extensive manual on how to interpret and use your lab results, A version control tool; Git/Github. It allows open and collaborative development of your projects,  The nice things about these tools is that they have been developed in an open source / open knowledge framework, therefore the only thing needed to learn them (besides a computer, access to the web and -OK- understanding of basic English) is time.\nThese technologies already had an impact on academic research once, with the advent of \u0026ldquo;big data\u0026rdquo;. They might have an impact again at the present day with the implementation of open hardware and open robotics.\nWhile learning those tools, I found these resources incredibly helpful :\n For the R language anything that Hadley Wickham does, The Mozilla Development Network gets you started with HTML and the web (I am still learning this) Markdown is just incredibly easy, just check the original guide or the Github Flavored Markdown guide What better place to get started with github then the Git Book itself? It\u0026rsquo;s at the same time easy and detailed.  I have not found yet the best guide for learning open hardware development.\nHave fun :)\n","date":1494892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494892800,"objectID":"8899a9a8c51ccf9100fe7ded01cefd2b","permalink":"/post/2017-05-16-enabling-tools/","publishdate":"2017-05-16T00:00:00Z","relpermalink":"/post/2017-05-16-enabling-tools/","section":"post","summary":"Scientists love programming, scientists love writing","tags":null,"title":"Enabling Tools","type":"post"},{"authors":["Florian Hahn","Otho Mantegazza","André Greiner","Peter Hegemann","Marion Eisenhut","Andreas PM Weber"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"227d1824fed3e27e82f912a5dbc3629d","permalink":"/publication/hahn-2017-efficient/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/hahn-2017-efficient/","section":"publication","summary":"","tags":null,"title":"An efficient visual screen for CRISPR/Cas9 activity in Arabidopsis thaliana","type":"publication"},{"authors":["Florian Hahn","Marion Eisenhut","Otho Mantegazza","Andreas PM Weber"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"e780816e19b81a47a66d2ff364c18e51","permalink":"/publication/hahn-2017-generation/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/hahn-2017-generation/","section":"publication","summary":"","tags":null,"title":"Generation of Targeted Knockout Mutants in Arabidopsis thaliana Using CRISPR/Cas9","type":"publication"},{"authors":["Peter K Lundquist","Otho Mantegazza","Anja Stefanski","Kai Stühler","Andreas PM Weber"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"5df8b02667ddd79ccd7da1aacd00da5e","permalink":"/publication/lundquist-2017-surveying/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/lundquist-2017-surveying/","section":"publication","summary":"","tags":null,"title":"Surveying the oligomeric state of Arabidopsis thaliana chloroplasts","type":"publication"},{"authors":["Mara L Schuler","Otho Mantegazza","Andreas PM Weber"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"0fca857f47654d339742b7ef4ec3d8d4","permalink":"/publication/schuler-2016-engineering/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/schuler-2016-engineering/","section":"publication","summary":"","tags":null,"title":"Engineering C4 photosynthesis into C3 chassis in the synthetic biology age","type":"publication"},{"authors":["Otho Mantegazza","Veronica Gregis","Marta Adelina Mendes","Piero Morandini","Márcio Alves-Ferreira","Camila M Patreze","Sarah M Nardeli","Martin M Kater","Lucia Colombo"],"categories":null,"content":"","date":1388530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388530800,"objectID":"f86e3a9783778d5406edcc2fc3fa9d5b","permalink":"/publication/mantegazza-2014-analysis/","publishdate":"2014-01-01T00:00:00+01:00","relpermalink":"/publication/mantegazza-2014-analysis/","section":"publication","summary":"","tags":null,"title":"Analysis of the arabidopsis REM gene family predicts functions during flower development","type":"publication"},{"authors":["Otho Mantegazza","Veronica Gregis","Matteo Chiara","Caterina Selva","Giulia Leo","David S Horner","Martin M Kater"],"categories":null,"content":"","date":1388530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388530800,"objectID":"1975a9b0d6eb88503db237bf341f1068","permalink":"/publication/mantegazza-2014-gene/","publishdate":"2014-01-01T00:00:00+01:00","relpermalink":"/publication/mantegazza-2014-gene/","section":"publication","summary":"","tags":null,"title":"Gene coexpression patterns during early development of the native Arabidopsis reproductive meristem: novel candidate developmental regulators and patterns of functional redundancy","type":"publication"}]